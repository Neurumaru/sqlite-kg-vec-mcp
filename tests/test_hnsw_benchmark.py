"""
HNSW ì¸ë±ìŠ¤ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸.
"""

import time
import os
import tempfile
import unittest
import numpy as np
import psutil
from typing import List, Dict, Any, Tuple
import matplotlib.pyplot as plt
import json
from pathlib import Path

import sys
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from sqlite_kg_vec_mcp.vector.hnsw import HNSWIndex


class PerformanceMetrics:
    """ì„±ëŠ¥ ì¸¡ì • ë©”íŠ¸ë¦­ í´ë˜ìŠ¤."""
    
    def __init__(self):
        self.metrics = {}
    
    def start_timer(self, name: str):
        """íƒ€ì´ë¨¸ ì‹œì‘."""
        self.metrics[f"{name}_start"] = time.time()
    
    def end_timer(self, name: str):
        """íƒ€ì´ë¨¸ ì¢…ë£Œ ë° ì†Œìš” ì‹œê°„ ê¸°ë¡."""
        if f"{name}_start" in self.metrics:
            duration = time.time() - self.metrics[f"{name}_start"]
            self.metrics[f"{name}_duration"] = duration
            return duration
        return None
    
    def record_memory(self, name: str):
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡."""
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        self.metrics[f"{name}_memory_mb"] = memory_mb
        return memory_mb
    
    def record_value(self, name: str, value: Any):
        """ì„ì˜ì˜ ê°’ ê¸°ë¡."""
        self.metrics[name] = value
    
    def get_metrics(self) -> Dict[str, Any]:
        """ëª¨ë“  ë©”íŠ¸ë¦­ ë°˜í™˜."""
        return self.metrics.copy()


class HNSWBenchmarkTest(unittest.TestCase):
    """HNSW ì¸ë±ìŠ¤ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸."""
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •."""
        self.temp_dir = tempfile.mkdtemp()
        self.metrics = PerformanceMetrics()
        self.results = []
        
        # í…ŒìŠ¤íŠ¸ ì„¤ì •
        self.dimensions = [128, 384, 768, 1536]  # ë‹¤ì–‘í•œ ë²¡í„° ì°¨ì›
        self.dataset_sizes = [1000, 5000, 10000, 50000]  # ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ í¬ê¸°
        self.search_k_values = [1, 5, 10, 50]  # ë‹¤ì–‘í•œ k ê°’
        
        # HNSW íŒŒë¼ë¯¸í„° ì„¤ì •
        self.hnsw_configs = [
            {"M": 16, "ef_construction": 200, "ef_search": 50},
            {"M": 32, "ef_construction": 400, "ef_search": 100},
            {"M": 64, "ef_construction": 800, "ef_search": 200},
        ]
    
    def tearDown(self):
        """í…ŒìŠ¤íŠ¸ ì •ë¦¬."""
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def generate_random_vectors(self, n: int, dim: int) -> np.ndarray:
        """ëœë¤ ë²¡í„° ìƒì„±."""
        vectors = np.random.randn(n, dim).astype(np.float32)
        # L2 ì •ê·œí™”
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        vectors = vectors / (norms + 1e-8)
        return vectors
    
    def generate_clustered_vectors(self, n: int, dim: int, n_clusters: int = 10) -> np.ndarray:
        """í´ëŸ¬ìŠ¤í„°ëœ ë²¡í„° ìƒì„± (ë” í˜„ì‹¤ì ì¸ ë°ì´í„°)."""
        vectors = []
        vectors_per_cluster = n // n_clusters
        
        for i in range(n_clusters):
            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì 
            center = np.random.randn(dim).astype(np.float32)
            center = center / np.linalg.norm(center)
            
            # í´ëŸ¬ìŠ¤í„° ì£¼ë³€ì˜ ë²¡í„°ë“¤
            cluster_vectors = np.random.randn(vectors_per_cluster, dim).astype(np.float32) * 0.3
            cluster_vectors += center
            
            # ì •ê·œí™”
            norms = np.linalg.norm(cluster_vectors, axis=1, keepdims=True)
            cluster_vectors = cluster_vectors / (norms + 1e-8)
            
            vectors.append(cluster_vectors)
        
        # ë‚¨ì€ ë²¡í„°ë“¤
        remaining = n - len(vectors) * vectors_per_cluster
        if remaining > 0:
            extra_vectors = self.generate_random_vectors(remaining, dim)
            vectors.append(extra_vectors)
        
        return np.vstack(vectors)
    
    def calculate_recall(self, true_neighbors: List[int], found_neighbors: List[int]) -> float:
        """ì¬í˜„ìœ¨(Recall) ê³„ì‚°."""
        if not true_neighbors or not found_neighbors:
            return 0.0
        
        true_set = set(true_neighbors)
        found_set = set(found_neighbors)
        intersection = true_set.intersection(found_set)
        
        return len(intersection) / len(true_set)
    
    def brute_force_search(self, query: np.ndarray, vectors: np.ndarray, k: int) -> List[int]:
        """ë¸Œë£¨íŠ¸ í¬ìŠ¤ ê²€ìƒ‰ (ì •ë‹µ ê¸°ì¤€)."""
        distances = np.linalg.norm(vectors - query, axis=1)
        indices = np.argsort(distances)[:k]
        return indices.tolist()
    
    def benchmark_index_construction(self, vectors: np.ndarray, config: Dict[str, Any]) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ êµ¬ì¶• ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬."""
        metrics = PerformanceMetrics()
        
        metrics.record_memory("before_construction")
        metrics.start_timer("construction")
        
        # HNSW ì¸ë±ìŠ¤ ìƒì„±
        index = HNSWIndex(
            space="cosine",
            dim=vectors.shape[1],
            ef_construction=config["ef_construction"],
            M=config["M"],
            index_dir=self.temp_dir
        )
        
        index.init_index(max_elements=len(vectors))
        
        # ë²¡í„° ì¶”ê°€
        for i, vector in enumerate(vectors):
            index.add_item(
                entity_type="test",
                entity_id=i,
                vector=vector
            )
        
        construction_time = metrics.end_timer("construction")
        metrics.record_memory("after_construction")
        
        # ef_search ì„¤ì •
        if hasattr(index.index, 'set_ef'):
            index.index.set_ef(config.get("ef_search", 50))
        
        result = {
            "construction_time": construction_time,
            "memory_before_mb": metrics.metrics["before_construction_memory_mb"],
            "memory_after_mb": metrics.metrics["after_construction_memory_mb"],
            "memory_used_mb": metrics.metrics["after_construction_memory_mb"] - metrics.metrics["before_construction_memory_mb"],
            "vectors_count": len(vectors),
            "dimension": vectors.shape[1],
            "config": config
        }
        
        return result, index
    
    def benchmark_search_performance(
        self, 
        index: HNSWIndex, 
        query_vectors: np.ndarray, 
        all_vectors: np.ndarray,
        k_values: List[int]
    ) -> Dict[str, Any]:
        """ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬."""
        results = {}
        
        for k in k_values:
            metrics = PerformanceMetrics()
            recalls = []
            
            metrics.start_timer(f"search_k{k}")
            
            for query in query_vectors:
                # HNSW ê²€ìƒ‰
                hnsw_results = index.search(query, k)
                hnsw_indices = [result[1] for result in hnsw_results]
                
                # ë¸Œë£¨íŠ¸ í¬ìŠ¤ ê²€ìƒ‰ (ì •ë‹µ)
                true_indices = self.brute_force_search(query, all_vectors, k)
                
                # ì¬í˜„ìœ¨ ê³„ì‚°
                recall = self.calculate_recall(true_indices, hnsw_indices)
                recalls.append(recall)
            
            search_time = metrics.end_timer(f"search_k{k}")
            
            results[f"k{k}"] = {
                "total_search_time": search_time,
                "avg_search_time_ms": (search_time / len(query_vectors)) * 1000,
                "queries_per_second": len(query_vectors) / search_time,
                "average_recall": np.mean(recalls),
                "min_recall": np.min(recalls),
                "max_recall": np.max(recalls),
                "recall_std": np.std(recalls)
            }
        
        return results
    
    def test_dimension_scaling_benchmark(self):
        """ì°¨ì› ì¦ê°€ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ë²¤ì¹˜ë§ˆí¬."""
        print("\nğŸ”¬ ì°¨ì› ìŠ¤ì¼€ì¼ë§ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        dataset_size = 10000
        query_size = 100
        config = self.hnsw_configs[1]  # ì¤‘ê°„ ì„¤ì • ì‚¬ìš©
        
        for dim in self.dimensions:
            print(f"\nğŸ“Š ì°¨ì›: {dim}")
            
            # ë°ì´í„° ìƒì„±
            vectors = self.generate_clustered_vectors(dataset_size, dim)
            query_vectors = self.generate_random_vectors(query_size, dim)
            
            # ì¸ë±ìŠ¤ êµ¬ì¶• ë²¤ì¹˜ë§ˆí¬
            construction_result, index = self.benchmark_index_construction(vectors, config)
            
            # ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
            search_results = self.benchmark_search_performance(
                index, query_vectors, vectors, [10]
            )
            
            result = {
                "dimension": dim,
                "dataset_size": dataset_size,
                "construction": construction_result,
                "search": search_results["k10"]
            }
            
            self.results.append(result)
            
            print(f"   êµ¬ì¶• ì‹œê°„: {construction_result['construction_time']:.2f}ì´ˆ")
            print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©: {construction_result['memory_used_mb']:.1f}MB")
            print(f"   ê²€ìƒ‰ ì†ë„: {search_results['k10']['queries_per_second']:.1f} QPS")
            print(f"   í‰ê·  ì¬í˜„ìœ¨: {search_results['k10']['average_recall']:.3f}")
    
    def test_dataset_size_scaling_benchmark(self):
        """ë°ì´í„°ì…‹ í¬ê¸° ì¦ê°€ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ë²¤ì¹˜ë§ˆí¬."""
        print("\nğŸ”¬ ë°ì´í„°ì…‹ í¬ê¸° ìŠ¤ì¼€ì¼ë§ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        dim = 384  # ê³ ì • ì°¨ì›
        query_size = 100
        config = self.hnsw_configs[1]  # ì¤‘ê°„ ì„¤ì • ì‚¬ìš©
        
        for size in self.dataset_sizes:
            print(f"\nğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {size:,}")
            
            # ë°ì´í„° ìƒì„±
            vectors = self.generate_clustered_vectors(size, dim)
            query_vectors = self.generate_random_vectors(query_size, dim)
            
            # ì¸ë±ìŠ¤ êµ¬ì¶• ë²¤ì¹˜ë§ˆí¬
            construction_result, index = self.benchmark_index_construction(vectors, config)
            
            # ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
            search_results = self.benchmark_search_performance(
                index, query_vectors, vectors, [10]
            )
            
            result = {
                "dataset_size": size,
                "dimension": dim,
                "construction": construction_result,
                "search": search_results["k10"]
            }
            
            self.results.append(result)
            
            print(f"   êµ¬ì¶• ì‹œê°„: {construction_result['construction_time']:.2f}ì´ˆ")
            print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©: {construction_result['memory_used_mb']:.1f}MB")
            print(f"   ê²€ìƒ‰ ì†ë„: {search_results['k10']['queries_per_second']:.1f} QPS")
            print(f"   í‰ê·  ì¬í˜„ìœ¨: {search_results['k10']['average_recall']:.3f}")
    
    def test_parameter_tuning_benchmark(self):
        """HNSW íŒŒë¼ë¯¸í„° íŠœë‹ ë²¤ì¹˜ë§ˆí¬."""
        print("\nğŸ”¬ íŒŒë¼ë¯¸í„° íŠœë‹ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        dataset_size = 20000
        dim = 384
        query_size = 100
        
        # ë°ì´í„° ìƒì„±
        vectors = self.generate_clustered_vectors(dataset_size, dim)
        query_vectors = self.generate_random_vectors(query_size, dim)
        
        for i, config in enumerate(self.hnsw_configs):
            print(f"\nğŸ“Š ì„¤ì • {i+1}: M={config['M']}, ef_construction={config['ef_construction']}, ef_search={config['ef_search']}")
            
            # ì¸ë±ìŠ¤ êµ¬ì¶• ë²¤ì¹˜ë§ˆí¬
            construction_result, index = self.benchmark_index_construction(vectors, config)
            
            # ê²€ìƒ‰ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
            search_results = self.benchmark_search_performance(
                index, query_vectors, vectors, [1, 10, 50]
            )
            
            result = {
                "config_index": i,
                "config": config,
                "dataset_size": dataset_size,
                "dimension": dim,
                "construction": construction_result,
                "search": search_results
            }
            
            self.results.append(result)
            
            print(f"   êµ¬ì¶• ì‹œê°„: {construction_result['construction_time']:.2f}ì´ˆ")
            print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©: {construction_result['memory_used_mb']:.1f}MB")
            
            for k in [1, 10, 50]:
                search_result = search_results[f"k{k}"]
                print(f"   k={k}: {search_result['queries_per_second']:.1f} QPS, ì¬í˜„ìœ¨ {search_result['average_recall']:.3f}")
    
    def test_comprehensive_benchmark(self):
        """ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬."""
        print("\nğŸ”¬ ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
        
        # ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ì™€ ìœ ì‚¬í•œ ì„¤ì •
        config = {"M": 32, "ef_construction": 400, "ef_search": 100}
        dataset_size = 50000
        dim = 768  # OpenAI embedding ì°¨ì›ê³¼ ìœ ì‚¬
        query_size = 1000
        
        print(f"ğŸ“Š ì„¤ì •: {dataset_size:,}ê°œ ë²¡í„°, {dim}ì°¨ì›")
        
        # í´ëŸ¬ìŠ¤í„°ëœ ë°ì´í„° ìƒì„± (ë” í˜„ì‹¤ì )
        vectors = self.generate_clustered_vectors(dataset_size, dim, n_clusters=20)
        query_vectors = self.generate_random_vectors(query_size, dim)
        
        # ì¸ë±ìŠ¤ êµ¬ì¶•
        print("ğŸ”¨ ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        construction_result, index = self.benchmark_index_construction(vectors, config)
        
        # ë‹¤ì–‘í•œ k ê°’ìœ¼ë¡œ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        print("ğŸ” ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        search_results = self.benchmark_search_performance(
            index, query_vectors, vectors, self.search_k_values
        )
        
        # ê²°ê³¼ ì €ì¥
        comprehensive_result = {
            "test_type": "comprehensive",
            "config": config,
            "dataset_size": dataset_size,
            "dimension": dim,
            "query_size": query_size,
            "construction": construction_result,
            "search": search_results
        }
        
        self.results.append(comprehensive_result)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nâœ… ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
        print(f"   ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œê°„: {construction_result['construction_time']:.2f}ì´ˆ")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {construction_result['memory_used_mb']:.1f}MB")
        print(f"   ì´ˆë‹¹ ë²¡í„° ì¶”ê°€: {dataset_size / construction_result['construction_time']:.0f} vectors/sec")
        
        print(f"\n   ê²€ìƒ‰ ì„±ëŠ¥:")
        for k in self.search_k_values:
            result = search_results[f"k{k}"]
            print(f"   k={k:2d}: {result['queries_per_second']:6.1f} QPS, ì¬í˜„ìœ¨ {result['average_recall']:.3f}")
    
    def save_benchmark_results(self):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥."""
        if not self.results:
            return
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        results_file = os.path.join(self.temp_dir, "hnsw_benchmark_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {results_file}")
        return results_file
    
    def generate_performance_report(self):
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±."""
        if not self.results:
            return
        
        print("\nğŸ“Š HNSW ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ")
        print("=" * 50)
        
        # ìµœê³  ì„±ëŠ¥ ì°¾ê¸°
        best_qps = 0
        best_recall = 0
        best_construction = float('inf')
        
        for result in self.results:
            if 'search' in result:
                if 'k10' in result['search']:
                    qps = result['search']['k10']['queries_per_second']
                    recall = result['search']['k10']['average_recall']
                    construction_time = result['construction']['construction_time']
                    
                    if qps > best_qps:
                        best_qps = qps
                    if recall > best_recall:
                        best_recall = recall
                    if construction_time < best_construction:
                        best_construction = construction_time
        
        print(f"ğŸ† ìµœê³  ê²€ìƒ‰ ì†ë„: {best_qps:.1f} QPS")
        print(f"ğŸ¯ ìµœê³  ì¬í˜„ìœ¨: {best_recall:.3f}")
        print(f"âš¡ ìµœë‹¨ êµ¬ì¶• ì‹œê°„: {best_construction:.2f}ì´ˆ")
        
        # ê¶Œì¥ ì„¤ì •
        print(f"\nğŸ’¡ ê¶Œì¥ ì„¤ì •:")
        print(f"   - ì†Œê·œëª¨ ë°ì´í„°ì…‹ (< 10K): M=16, ef_construction=200")
        print(f"   - ì¤‘ê°„ ë°ì´í„°ì…‹ (10K-50K): M=32, ef_construction=400")
        print(f"   - ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ (> 50K): M=64, ef_construction=800")
        print(f"   - ì •í™•ë„ ìš°ì„ ì‹œ: ef_searchë¥¼ ë†’ê²Œ ì„¤ì • (100-200)")
        print(f"   - ì†ë„ ìš°ì„ ì‹œ: ef_searchë¥¼ ë‚®ê²Œ ì„¤ì • (50-100)")
    
    def run_all_benchmarks(self):
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰."""
        print("ğŸš€ HNSW ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        print("=" * 50)
        
        # ê°œë³„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        self.test_dimension_scaling_benchmark()
        self.test_dataset_size_scaling_benchmark()
        self.test_parameter_tuning_benchmark()
        self.test_comprehensive_benchmark()
        
        # ê²°ê³¼ ì €ì¥ ë° ë³´ê³ ì„œ ìƒì„±
        self.save_benchmark_results()
        self.generate_performance_report()
        
        print(f"\nâœ… ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ì´ {len(self.results)}ê°œì˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")


class HNSWPerformanceTest(HNSWBenchmarkTest):
    """ë‹¨ì¼ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (CI/CDì—ì„œ ë¹ ë¥¸ ì‹¤í–‰ìš©)."""
    
    def test_quick_performance_check(self):
        """ë¹ ë¥¸ ì„±ëŠ¥ í™•ì¸ í…ŒìŠ¤íŠ¸."""
        print("\nâš¡ ë¹ ë¥¸ HNSW ì„±ëŠ¥ í™•ì¸...")
        
        # ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        vectors = self.generate_random_vectors(1000, 128)
        query_vectors = self.generate_random_vectors(10, 128)
        config = {"M": 16, "ef_construction": 200, "ef_search": 50}
        
        # ì¸ë±ìŠ¤ êµ¬ì¶•
        construction_result, index = self.benchmark_index_construction(vectors, config)
        
        # ê²€ìƒ‰ ì„±ëŠ¥
        search_results = self.benchmark_search_performance(
            index, query_vectors, vectors, [10]
        )
        
        # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
        self.assertLess(construction_result['construction_time'], 10.0, "ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œê°„ì´ ë„ˆë¬´ ê¸¸ìŒ")
        self.assertGreater(search_results['k10']['queries_per_second'], 100, "ê²€ìƒ‰ ì†ë„ê°€ ë„ˆë¬´ ëŠë¦¼")
        self.assertGreater(search_results['k10']['average_recall'], 0.8, "ì¬í˜„ìœ¨ì´ ë„ˆë¬´ ë‚®ìŒ")
        
        print(f"âœ… ì„±ëŠ¥ í™•ì¸ ì™„ë£Œ:")
        print(f"   êµ¬ì¶• ì‹œê°„: {construction_result['construction_time']:.2f}ì´ˆ")
        print(f"   ê²€ìƒ‰ ì†ë„: {search_results['k10']['queries_per_second']:.1f} QPS")
        print(f"   ì¬í˜„ìœ¨: {search_results['k10']['average_recall']:.3f}")


if __name__ == "__main__":
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = HNSWBenchmarkTest()
    benchmark.setUp()
    
    try:
        benchmark.run_all_benchmarks()
    finally:
        benchmark.tearDown()